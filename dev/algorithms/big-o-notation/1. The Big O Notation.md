
An idea of which method to solve certain problem is the best.
It allows us to talk formally about how the runtime of an algorithm grows as the inputs grow.

- It’s important to have a precise vocabulary to talk about how the code performs
- Useful of discussing trade-offs between different approaches
- When the code slows down or crashes, identifying parts of the code that are inefficient can help us find pain points in our applications

Suppose we want to write a function that calculates the sum of all numbers from 1 up to (and including) some number *n*.

```javascript
function addUpTo(n){ 
	let total = 0;
	for (let i = 0; i <= n; i++){
		total += i;
	} 
	return total;
}
addUpTo(4); // 10 (1 + 2 + 3 + 4)
```

Here’s a more concise way.

```javascript
function addUpTo(n){
	return n * (n + 1) / 2;
}
```

Which one is better????? Then, what does “better” mean?? 
- Faster?
- Less memory-intensive?
- More readable?

## Big O Definition

We say that an algorithm is **O(f(n))** if the number of simple operations the computer has to do is eventually less than a constant times **f(n)**, as **n** increases.
- f(n) could be linear (f(n) = n)
- f(n) could be quadratic (f(n) = n^2)
- f(n) could be constant (f(n) = 1)
- f(n) could be something entirely different

## Time Complexity

Just a analysis of an algorithm as the size of the inputs increases.
- O(n)
- O(n^2)
- O(1)

## Space Complexity

We can also use big O notation to analyze **space complexity**: how much additional memory do we need to allocate in order to run the code in our algorithm.

Sometimes you'll hear the term **auxiliary space complexity** to refer to space required by the algorithm, not including space taken up by the inputs.

### Rules of Thumb

- Most primitives (booleans, numbers, undefined, null) are constant space.
- Strings require O(n) space (where *n* is the string length)
- Reference types are generally O(n), where *n* is the length (for arrays) or the number of keys (for objects)

#### Example 1:
```js
function sum(arr){
	let total = 0;
	for (let i = 0; i < arr.length; i++){
		total += arr[i];
	}
	return total;
}
```
We have two variables: `total` and `i`. These two variables will NOT grow as the size of `arr` grows, because we are not creating a new variable, but we are just adding to the variable. They will always be the same size. So we call this **O(1) space**.

#### Example 2:
```js
function double(arr){
	let newArr = [];
	for (let i = 0; i < arr.length; i++){
		newArr.push(2 * arr[i]);
	}
	return newArr;
}
```
We have the `newArr` variable that expects elements to be pushed in. The size of this variable will grow in proportion to the size `arr`. So the size of this function is **O(n) space**.

## Logarithms

We've encountered some of the most common complexities: O(1), O(n), O(n^2).
Sometimes big O expressions involve more complex mathematical expressions.

### What's a log again?

log<sub>2</sub>(8) = 3 --> 2 to what power is 8? 2<sup>3</sup> = 8.
log<sub>2</sub>(value) = exponent --> 2<sup>exponent</sup> = value.
We can say log === log<sub>2</sub> 
The logarithm of a number roughly measures the number of times you can divide that number by 2 **before you get a value that's less than or equal to one**.

### Logarithm Complexity
Logarithm time complexity is great!